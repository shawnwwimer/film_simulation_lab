{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-endorsement",
   "metadata": {},
   "source": [
    "# Monte Carlo example: complicated dice rolling\n",
    "* Roll four dice\n",
    "* Reroll any ones\n",
    "* Take the sum of the three largest\n",
    "\n",
    "We can approximate the theoretical distribution by the distribution of actual events simulated with the Monte Carlo method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "\n",
    "# sample the potential inputs\n",
    "samples = 1000 # large numbers of samples are better\n",
    "total_rolls = samples*4*2 # rolls and potential rerolls for four dice\n",
    "all_rolls = rng.integers(1, 6, endpoint=True, size=total_rolls) # bounded within [1,6] \n",
    "\n",
    "# process the rolls\n",
    "dice = all_rolls[:total_rolls//2] # take the first half for all of the dice\n",
    "rerolled = dice.copy() # copy the array so can plot the initial rolls later\n",
    "for i in range(total_rolls//2):\n",
    "    if rerolled[i] == 1:\n",
    "        rerolled[i] = all_rolls[i+total_rolls//2] # use the rerolled value if the first roll was 1\n",
    "d1 = rerolled[:total_rolls//8] # the first eighth is the first die\n",
    "d2 = rerolled[total_rolls//8:2*total_rolls//8] # the second eighth is the second die\n",
    "d3 = rerolled[2*total_rolls//8:3*total_rolls//8] # the third eighth is the third die\n",
    "d4 = rerolled[3*total_rolls//8:4*total_rolls//8] # the fourth eighth is the last die\n",
    "\n",
    "# how are the rolls distributed between then and now?\n",
    "print(\"\\t1\\t2\\t3\\t4\\t6\\t6\")\n",
    "print(\"1\", end='')\n",
    "for i in range(1,7):\n",
    "    print(f'\\t{np.count_nonzero(dice==i)/samples/4:.3f}', end='')\n",
    "print('\\n')\n",
    "print(\"2\", end='')\n",
    "for i in range(1,7):\n",
    "    print(f'\\t{np.count_nonzero(rerolled==i)/samples/4:.3f}', end='')\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "# stack the dice so they look like this:\n",
    "# d11 d12 d13 ... d1n\n",
    "# d21 d22 d23 ... d2n\n",
    "# d31 d32 d33 ... d3n\n",
    "# d41 d42 d43 ... d4n\n",
    "stack = np.vstack([d1, d2, d3, d4]) \n",
    "\n",
    "# then sum each column and subtract the minimum value in that column\n",
    "s = np.sum(stack, axis=0) - np.min(stack, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# plot the initial rolls, the rolls after rerolls, and the eventual sum\n",
    "fig, ax = plt.subplot_mosaic([['a', 'c'], ['b', 'c']], figsize=(10,8), dpi=150)\n",
    "ax['a'].hist(dice, bins=6)\n",
    "ax['b'].hist(rerolled, bins=6)\n",
    "ax['c'].hist(s, bins=range(3, 20), cumulative=True, density=True)\n",
    "ax['c'].hist(s, bins=range(3, 20), cumulative=False, density=True)\n",
    "ax['c'].legend(['CDF', 'PDF'])\n",
    "ax['a'].set_ylabel('First throw', fontsize=12)\n",
    "ax['b'].set_ylabel('Rerolled', fontsize=12)\n",
    "ax['c'].set_xlabel('Sum', fontsize=12)\n",
    "ax['a'].set_xlabel('Number', fontsize=12)\n",
    "ax['b'].set_xlabel('Number', fontsize=12)\n",
    "ax['c'].set_ylabel('Counts', fontsize=12)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-schedule",
   "metadata": {},
   "source": [
    "## Outcome\n",
    "The first rolls are very uniform. Rerolling the ones drastically reduces the number of them in the final count (~1/6 -> ~1/36). The distributions of rolls isn't particularly uniform but the distribution of sums shows a clear tendency towards higher sums. Let's look at the effects of sample sizes on the resulting distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "\n",
    "results = []\n",
    "sample_nums = [100, 10000, 1000000, 10000000]\n",
    "for samples in sample_nums:\n",
    "    total_rolls = samples*4\n",
    "    all_rolls = rng.integers(1, 6, endpoint=True, size=total_rolls)\n",
    "\n",
    "    dice = all_rolls[:total_rolls//2]\n",
    "    rerolled = dice.copy()\n",
    "    for i in range(total_rolls//2):\n",
    "        if rerolled[i] == 1:\n",
    "            rerolled[i] = all_rolls[i+total_rolls//2]\n",
    "    d1 = rerolled[:+total_rolls//8]\n",
    "    d2 = rerolled[total_rolls//8:2*total_rolls//8]\n",
    "    d3 = rerolled[2*total_rolls//8:3*total_rolls//8]\n",
    "    d4 = rerolled[3*total_rolls//8:4*total_rolls//8]\n",
    "    stack = np.vstack([d1, d2, d3, d4])\n",
    "    results.append(np.sum(stack, axis=0) - np.min(stack, axis=0))\n",
    "\n",
    "fig, ax = plt.subplot_mosaic([['a', 'b'], ['c', 'd']], figsize=(10,8), dpi=150)\n",
    "names = ['a', 'b', 'c', 'd']\n",
    "for i in range(4):\n",
    "    ax[names[i]].hist(results[i], bins=range(3, 20))\n",
    "    ax[names[i]].set_xlabel('Sum', fontsize=12)\n",
    "    ax[names[i]].set_ylabel('Counts', fontsize=12)\n",
    "    ax[names[i]].legend([sample_nums[i]])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-agency",
   "metadata": {},
   "source": [
    "## Result\n",
    "We can see that a small sample size does not lead to a good distribution and that larger sample sizes converge. This is related to both the 'law of large numbers' and the 'central limit theorem' from probability theory and statistics. In complicated problems or statistics for small subsets of a larger population, convergence will take longer.\n",
    "\n",
    "However, low sample sizes don't lack utility. Even at 100 samples we can see a clear bias towards 12-14. 10000 samples still give a good idea of the shape and may be 'good enough' for some purposes. 1000000 and 10000000 lead to very similar distributions, indicating that we've likely come pretty close to the real distribution. In that case, why sample 10x as much, taking 10x as long, for limited gain? \n",
    "\n",
    "Simple: for higher accuracy. Convergence behaves as $\\sqrt{n}$ so a larger sample size will give lower error, but the error decreases less and less for higher $n$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
